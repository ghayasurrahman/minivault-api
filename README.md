# MiniVault API: Local Model Integration with Ollama

This project simulates a core **ModelVault** feature: a local API that runs an AI model (Llama2) to respond to user prompts â€” completely offline. The API serves a `POST /generate` endpoint where you can send text prompts and receive responses generated by the model.

Additionally, there is a `/status` endpoint that provides system metrics like memory usage, CPU count, and server uptime.

## Project Setup

### Prerequisites

Before you begin, make sure you have the following installed:

- **Node.js** (v16.x or higher)
- **Ollama** (For local AI model execution)

### 1. Extract the Project

Once you receive the project as a zip file, extract it to your desired directory.

### 2. Install Dependencies

Navigate to the project folder and install the required npm dependencies:

```bash
cd /path/to/minivault-api
npm install


# MiniVault API & CLI

A lightweight local API and CLI interface for interacting with your Ollama-powered LLMs (like LLaMA 2) directly from your terminal. Designed for developers who want to run models locally and get quick responses via the command line.

---

## Features

- Run prompts using local Ollama models (e.g., llama2)
- Express-based API with a `/generate` endpoint
- Chunked streaming support
- Minimal CLI: minivault "Your prompt here"
- JSONL logging for all interactions

---

## Installation & Setup

Follow these steps to set up the MiniVault API and CLI on your local machine:

### 1. Clone the repository

```bash
git clone https://github.com/your-username/minivault-api.git
cd minivault-api

# minivault-api
